# Live Stand: "Диалоговый ИИ-Ассистент"

## 1. Видение и Цель

Цель этого проекта — демонстрация прототипа голосового ИИ-ассистента нового поколения, способного вести естественный, бесшовный диалог в реальном времени. Мы показываем, что технология позволяет уйти от роботизированных ответов и раздражающих пауз, создавая эффективный инструмент для бизнес-коммуникаций.

**Ключевые "Вау-эффекты":**

*   **Мгновенная реакция:** Ассистент отвечает на стандартные вопросы без задержки.
*   **"Мышление в реальном времени":** Ответ на сложные вопросы начинается практически сразу, создавая иллюзию живого мыслительного процесса.
*   **Полный контроль диалога:** Пользователь может в любой момент перебить ассистента, и тот мгновенно уступит, как вежливый собеседник (Barge-in).
*   **Премиальное качество голоса:** Голос ассистента звучит естественно и убедительно.

---

## 2. Архитектура Системы

Система построена на принципах скорости, интеллекта и качества. Каждый модуль вносит свой вклад в финальный результат.

![Архитектура](https://i.imgur.com/your-architecture-diagram.png) <!-- Замените на реальную диаграмму -->

| Компонент | Технология | Бизнес-обоснование |
| :--- | :--- | :--- |
| **Frontend** | React / Next.js | Быстрый старт и фокус на визуализации "мыслительного процесса" ассистента. |
| **Media Server**| **LiveKit** | Гарантирует надежную WebRTC-связь с низкой задержкой для бесшовной передачи голоса. |
| **Orchestrator**| Python (Asyncio) | Гибкое ядро, управляющее лучшими AI-сервисами на рынке. |
| **In-Memory Cache**| Redis | Обеспечивает мгновенный доступ к готовым аудио-ответам, создавая "вау-эффект". |
| **STT** | Yandex SpeechKit | Лучшее на рынке распознавание русской речи с минимальной задержкой. |
| **LLM** | OpenAI GPT-4o mini | Позволяет ассистенту вести осмысленный диалог, а не просто отвечать по скрипту. |
| **TTS** | ElevenLabs | Обеспечивает премиальное, естественное звучание голоса, повышая доверие. |

---

## 3. Быстрый старт для разработчика

### 3.1. Установка зависимостей

1.  **Настройте окружение:**
    ```bash
    python -m venv venv
    source venv/bin/activate
    ```

2.  **Установите `requirements.txt`:**
    ```bash
    pip install -r requirements.txt
    ```

3.  **Настройте переменные окружения:**
    Скопируйте `env.example` в `.env` и заполните своими API-ключами и настройками.
    ```bash
    cp env.example .env
    # nano .env
    ```

### 3.2. Локальный запуск LiveKit и Redis

Для полноценной работы необходим запущенный сервер LiveKit и Redis.
Используйте Docker для быстрого развертывания:

```bash
# Запуск LiveKit
docker run --rm -it -p 7880:7880 -p 7881:7881 livekit/livekit-server --dev

# Запуск Redis
docker run --rm -d -p 6379:6379 --name redis-cache redis
```

Это запустит LiveKit в режиме разработки. `API Key` и `Secret` будут выведены в консоль — их нужно будет добавить в ваш `.env` файл.

---

## 4. Структура проекта и Описание модулей

Ниже представлено описание ключевых директорий и их содержимого.

```
project_root/
├─ README.md              # Этот файл
├─ .env.example           # Образец переменных окружения
├─ requirements.txt       # Зависимости Python
├─ configs/               # Все конфигурационные файлы
├─ scripts/               # Вспомогательные скрипты для сборки и тестов
├─ infra/                 # Конфигурация инфраструктуры (логи, метрики, Redis)
├─ domain/                # Общие модели данных и интерфейсы
├─ cache/                 # Модуль кэширования (Redis)
├─ llm/                   # Логика работы с Large Language Models (OpenAI)
├─ flow_engine/           # Управление диалоговыми сценариями
├─ intent_classifier/     # Распознавание намерений пользователя
├─ stt_yandex/            # Распознавание речи (Speech-to-Text)
├─ tts_manager/           # Синтез речи (Text-to-Speech)
├─ orchestrator/          # "Дирижер" всего процесса
└─ webapi/                # WebSocket API для взаимодействия с Frontend
```

### Детальное описание модулей:

*   **`configs/`**: Содержит всю статическую конфигурацию:
    *   `config.yml`: Глобальные настройки (ключи, таймауты).
    *   `prompts.yml`: Системные промпты для LLM.
    *   `dialogue_map.json`: Карта диалоговых состояний и переходов.
    *   `goals.json`: Цели и параметры для `FlowEngine`.

*   **`scripts/`**: Набор утилит для подготовки и обслуживания проекта:
    *   `load_static_audio.py`: Загружает предзаписанные WAV-файлы в Redis-кэш. Используется для "прогрева" кэша статическими аудио-ответами, чтобы они были доступны с минимальной задержкой.
    *   `prepare_embeddings.py`: Создает векторные представления (эмбеддинги) фраз из `dialogue_map.json`. Это необходимо для работы `IntentClassifier`, который использует семантический поиск для определения намерений пользователя.
    *   `validate_dialogue_map.py`: Проверяет корректность ссылок и полей в `dialogue_map.json`, чтобы избежать ошибок во время выполнения диалога.
    *   `gen_token.py`: Генерирует JWT-токены для подключения к LiveKit.
    *   `benchmark_embed.py`: Скрипт для замера производительности эмбеддинг-модели.
    *   `download_model.py`: Скрипт для скачивания моделей из HuggingFace.

*   **`domain/`**: Центральное место для определения общих структур данных (`models.py`) и абстрактных интерфейсов (`interfaces/`). Это позволяет избежать циклических зависимостей и обеспечивает слабую связанность между модулями.

*   **`cache/`**: Реализация кэша на базе Redis. Хранит три типа данных: статические аудио-фрагменты, кэш TTS-синтеза и кэш суммаризаций диалогов от LLM.

*   **`intent_classifier/`**: Определяет намерение пользователя (`intent`) по его речи. Использует локальную ONNX-модель для быстрого семантического поиска по заранее подготовленным эмбеддингам.

*   **`flow_engine/`**: "Мозг" диалоговой системы. Управляет задачами, слотами и сценариями на основе `goals.json` и `dialogue_map.json`. Это stateless-синглтон, который принимает текущее состояние сессии и событие, а на выходе отдает новое состояние.

*   **`stt_yandex/`**: Модуль для потокового распознавания речи через Yandex SpeechKit. Обеспечивает получение как промежуточных (`partial`), так и финальных (`final`) результатов распознавания.

*   **`tts_manager/`**: Отвечает за синтез речи через ElevenLabs, используя гибридный подход (HTTP для коротких фраз, WebSocket для потоковых ответов LLM).

*   **`llm/`**: Управляет взаимодействием с OpenAI, включая управление контекстом диалога, промпт-инжиниринг и обработку потоковых ответов.

*   **`orchestrator/`**: "Дирижер" звонка. Координирует работу всех модулей: принимает аудио от `LiveKit`, отправляет его в `STT`, получает результат, передает в `IntentClassifier` и `FlowEngine`/`LLM`, получает аудио от `TTS` или `Cache` и отправляет его обратно в `LiveTKit`. На каждый звонок создается свой экземпляр Оркестратора.

*   **`webapi/`**: Точка входа в приложение. Реализует WebSocket сервер на FastAPI, который принимает подключения от клиентов, создает экземпляр `Orchestrator` на каждый звонок и управляет его жизненным циклом.

---

## 5. API визуализации (WebSocket)

API спроектировано для того, чтобы Frontend мог в реальном времени показывать "мысли" ассистента.

*   **Отправка на сервер:** `start_demo_call`, `user_interrupted`.
*   **Получение от сервера:**
    *   `demo_call_started`: Сессия началась.
    *   `bot_mind_state_update`: **Ключевое сообщение** с текущим состоянием (`IDLE`, `LISTENING`, `ANALYZING_SPEECH`, `CHECKING_CACHE`, `ROUTING_TO_LLM`, `GENERATING_SPEECH`, `SPEAKING`).
    *   `llm_text_chunk`: Потоковая передача генерируемого текста от LLM.
    *   `error_occurred`: Сообщение об ошибке.