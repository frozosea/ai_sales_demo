### **Резюме проекта: Live Stand "Диалоговый ИИ-Ассистент"**

#### **1. Видение и цель демонстрации**

Цель "live stand" — продемонстрировать прототип голосового ассистента нового поколения, способного вести диалог на уровне, неотличимом от живого общения. Мы покажем, что технология позволяет уйти от роботизированных ответов и раздражающих пауз, создавая по-настоящему эффективный и естественный инструмент для бизнес-коммуникаций.

**Ключевые "вау-эффекты" для демонстрации:**
* **Мгновенная реакция:** Ассистент отвечает на стандартные вопросы без малейшей задержки.
* **"Мышление в реальном времени":** На сложные вопросы ответ начинается практически сразу, создавая иллюзию живого мыслительного процесса.
* **Полный контроль диалога:** Пользователь может в любой момент перебить ассистента, и тот мгновенно уступит, как вежливый собеседник.
* **Премиальное качество голоса:** Голос ассистента звучит естественно и убедительно, что критически важно для имиджа компании.

#### **2. Архитектура "под капотом"**

Для достижения такого уровня производительности система построена на трех китах: **скорость, интеллект и качество**. Каждый модуль вносит свой вклад в финальный результат.

* **Frontend (Демонстрационный стенд):**
    * **Описание:** Интерактивный веб-интерфейс, который не просто позволяет позвонить ассистенту, но и **визуализирует его "мыслительный процесс" в реальном времени**. Показывает, слушает ли ассистент, думает или говорит, делая магию технологии наглядной.
    * **Роль в демо:** Визуализация скорости и отзывчивости системы.

* **Media Server (Транспортный узел):**
    * **Описание:** Высокопроизводительный медиа-сервер, работающий как цифровая "нервная система", передающая голос между клиентом и "мозгом" ассистента с минимально возможной задержкой.
    * **Роль в демо:** Обеспечивает кристально чистую и бесперебойную передачу голоса.

* **Orchestrator (Ядро принятия решений):**
    * **Описание:** Центральный "мозг" ассистента. Он мгновенно анализирует запрос и выбирает оптимальный путь для ответа: отдать ли заготовленный ответ из кэша за доли секунды или подключить мощный интеллект GPT для решения нестандартной задачи.
    * **Роль в демо:** Именно здесь реализуются ключевые трюки: упреждающее выполнение, агрессивное EOU и управление диалогом.

* **In-Memory Cache (Модуль мгновенных ответов):**
    * **Описание:** Сверхбыстрая "краткосрочная память" ассистента. Хранит готовые к воспроизведению аудио-ответы на самые частые вопросы.
    * **Роль в демо:** Обеспечивает мгновенную реакцию на стандартные фразы.

#### **3. Технологический стек и его бизнес-обоснование**

| Компонент | Технология | Зачем это нужно для демо? (Бизнес-обоснование) |
| :--- | :--- | :--- |
| **Frontend** | React / Next.js (на базе Vapi) | **Быстрый старт:** Позволяет не тратить время на разработку "с нуля", а сразу сфокусироваться на визуализации. |
| **Media Server** | **Mediasoup** | **Надежность:** Гарантирует отсутствие задержек и искажений звука, что критично для живой демонстрации. |
| **Orchestrator** | **Python (Asyncio)** | **Гибкость:** Позволяет легко подключать и управлять лучшими AI-сервисами на рынке. |
| **In-Memory Cache**| **Redis** | **Скорость:** Обеспечивает мгновенный доступ к готовым ответам, создавая "вау-эффект" на старте диалога. |
| **STT** | **Yandex SpeechKit** | **Точность и скорость:** Лучшее на рынке распознавание русской речи с минимальной задержкой. |
| **LLM** | **OpenAI GPT-4o mini** | **Интеллект:** Позволяет ассистенту вести осмысленный диалог, а не просто отвечать по скрипту. |
| **TTS** | **ElevenLabs** | **Качество и имидж:** Обеспечивает премиальное, естественное звучание голоса, повышая доверие к ассистенту. |

---

### **4. API для Live Stand: WebSocket API визуализации**

API спроектировано для того, чтобы Frontend мог в реальном времени показывать "мысли" ассистента, делая демонстрацию более наглядной и впечатляющей.

#### **Сообщения от клиента к серверу (Frontend -> Orchestrator)**

* **`start_demo_call`**
    * **Описание:** Начинает демонстрационный звонок.
    * **Payload:** `{ "user_name": "Генеральный Директор" }`

* **`user_interrupted`**
    * **Описание:** Сигнал о том, что пользователь перебил ассистента.
    * **Payload:** `{}`

#### **Сообщения от сервера к клиенту (Orchestrator -> Frontend)**

* **`demo_call_started`**
    * **Описание:** Подтверждение начала сессии.
    * **Payload:** `{ "session_id": "xyz-123" }`

* **`bot_mind_state_update`**
    * **Описание:** **Ключевое сообщение для визуализации.** Отправляется при каждом изменении состояния ассистента.
    * **Payload:** `{ "state": "STATE_NAME", "details": "optional text" }`
    * **Возможные состояния (`STATE_NAME`):**
        * `IDLE`: Ассистент в режиме ожидания.
        * `LISTENING`: Ассистент слушает речь пользователя. (UI: иконка микрофона, звуковая волна).
        * `ANALYZING_SPEECH`: Пользователь замолчал, идет финальный анализ фразы (EOU). (UI: иконка "мозга", короткая анимация).
        * `CHECKING_CACHE`: Запрос отправлен в кэш мгновенных ответов. (UI: иконка "базы данных", анимация поиска).
        * `ROUTING_TO_LLM`: Кэш не помог, запрос уходит в GPT. (UI: иконка "облака с молнией").
        * `GENERATING_SPEECH`: Ответ от GPT получен, идет потоковый синтез голоса в ElevenLabs. (UI: иконка "звуковых волн", анимация генерации).
        * `SPEAKING`: Ассистент говорит. (UI: иконка "динамика", анимация голоса).

* **`error_occurred`**
    * **Описание:** Сообщает об ошибке (для отладки на стенде).
    * **Payload:** `{ "message": "TTS service unavailable" }`


Как ускорить: https://rnikhil.com/2025/05/18/how-to-reduce-latency-voice-agents#:~:text=then%20take%20up%20with%20my,can%20play%20with%20it%20at 


Отлично, теперь все на своих местах. Мы прояснили все ключевые моменты. На основе ваших ответов я подготовил финальный, детализированный архитектурный план.

Это именно тот документ, который можно использовать как техническое задание для команды разработки.

---

### **Архитектурный план демо-версии: "Live Stand"**

#### **1. Цели и задачи**

Создание демонстрационного стенда голосового ИИ-ассистента с целевой задержкой **< 800 мс** (от конца речи пользователя до первого аудио-токена ответа). Демонстрация призвана произвести "вау-эффект" на руководство за счет скорости, естественности диалога и возможности прерывания (barge-in).

#### **2. Логгирование и метрики**

Для анализа производительности и выявления "узких мест" в `orchestrator.py` будет вестись лог-файл с таймстампами для каждого диалога.

* **`t1`**: Время получения первого аудио-байта от клиента.
* **`t2`**: Время получения финального текста от **Yandex STT**.
* **`t3`**: Время отправки запроса в **Elasticsearch**.
* **`t4`**: Время отправки запроса в **OpenAI LLM**.
* **`t5_cache`** или **`t5_tts`**: Время получения первого аудио-чанка (из **Redis-кэша** или от **ElevenLabs TTS**). Логируем с разными суффиксами для разделения путей.
* **`t6`**: Время отправки первого аудио-чанка клиенту.

Это позволит рассчитывать точную задержку каждого компонента в конвейере.

#### **3. Архитектура и модули (финальная версия)**

* **`data.json`**: Исходный файл для конфигурации быстрых ответов. Содержит вопросы, синонимы, путь к WAV-файлу и `redis_key`.
* **`load_data.py`**: Скрипт для разовой загрузки данных из `data.json` в **Elasticsearch**. Формирует индекс с синонимами для полнотекстового поиска.
* **`cache.py` (Модуль и интерфейс кэша)**:
    * Предоставляет методы для работы с **Redis**.
    * Содержит логику для **динамического кэширования** ответов от LLM (семантический кэш).
    * Используется скриптом `load_to_redis.py` для выполнения своей задачи.
* **`load_to_redis.py`**: Скрипт, который при старте использует `cache.py` для чтения WAV-файлов, их чанкования и загрузки в **Redis** в виде списков байтов.
* **`ping.py`**: Сервисный скрипт для проверки доступности всех внешних (Yandex, OpenAI, ElevenLabs) и внутренних (Elasticsearch, Redis) сервисов.
* **`cold_start.py`**: Скрипт для "прогрева" API. Отправляет по одному пустому запросу ко всем внешним сервисам для минимизации задержки первого реального запроса.
* **`fillers/`**: Папка с короткими WAV-файлами ("Секундочку...", "Так...") для заполнения пауз, если LLM думает дольше заданного таймаута (например, > 300 мс).
* **`media_server/`**: Модуль для конфигурации и взаимодействия с **Mediasoup**.
* **`stt_yandex.py`**: Модуль для потокового распознавания речи через **Yandex SpeechKit v3 gRPC API**.
* **`tts_11labs.py`**: Модуль для потокового синтеза речи через **ElevenLabs WebSocket API**.
* **`llm.py`**: Модуль для взаимодействия с **OpenAI API**, использующий **стриминг через Server-Sent Events (SSE)** с параметром `stream=True`.
* **`elastic.py`**: Модуль для взаимодействия с Elasticsearch, инкапсулирующий логику гибридного поиска.
* **`orchestrator.py`**: Центральный асинхронный модуль (`asyncio`), реализующий основную бизнес-логику.
* **`webserver.py`**: Сервер на базе WebSocket, реализующий наше "API для Live Stand".

#### **4. Ключевые процессы и пайплайны**

**Пайплайн "Быстрого ответа" (параллельная гонка):**

1.  Оркестратор получает финальный текст от STT (`t2`).
2.  **Одновременно** запускаются два процесса:
    * **Процесс А (Поиск по ключевым словам):** Запрос в Elasticsearch (`t3`). Возвращает кандидатов на основе текстового совпадения.
    * **Процесс Б (Векторный поиск):** Запрос в **Yandex Foundation Models API** для получения эмбеддинга текста.
3.  **Анализ результатов:**
    * Оркестратор ждет завершения обоих процессов (с таймаутом, например, 200 мс на весь поиск).
    * Сначала проверяются кандидаты из Elasticsearch. Если находится совпадение с высоким скором — немедленно отдается ответ из Redis-кэша.
    * Если скор низкий, используется эмбеддинг из Процесса Б для расчета косинусного сходства с эмбеддингами кандидатов. Если `similarity > 0.90` — отдается ответ из кэша.
4.  **Компромисс:** API Яндекса для векторизации не потоковое. Это означает, что Процесс Б будет самым медленным в этой гонке (около 150-300 мс). Таким образом, **минимальная задержка на принятие решения по "быстрому пути" будет ограничена скоростью работы API векторизации**. Это плата за высокую точность поиска.

**Пайплайн "Генеративного ответа":**

1.  Если "быстрый путь" не дал результата, Оркестратор отправляет запрос в OpenAI LLM (`t4`).
2.  Как только от LLM приходит первый текстовый токен (`llm_text_chunk`), Оркестратор **разветвляет (fan-out)** его:
    * Отправляет токен в **`tts_11labs.py`** для начала синтеза речи.
    * Отправляет токен в **`webserver.py`** для отправки на фронтенд и анимации текста.
3.  Этот процесс продолжается до конца генерации ответа.

#### **5. API для Live Stand: WebSocket API (финальная версия)**

* **Сообщения от клиента (Frontend -> Orchestrator):** `start_demo_call`, `user_interrupted`. (Без изменений).

* **Сообщения от сервера (Orchestrator -> Frontend):**
    * `demo_call_started` (Без изменений).
    * `bot_mind_state_update` (Без изменений, состояния остаются актуальными).
    * **НОВОЕ СООБЩЕНИЕ:**
        * **`llm_text_chunk`**
            * **Описание:** Отправляет на фронтенд очередной текстовый токен ответа от LLM для анимации "печатающегося" текста.
            * **Payload:** `{ "chunk": "Здравствуйте" }`
    * `error_occurred` (Без изменений).


### Tree of project
```shell
project_root/
├─ README.md                                  # Как запускать демо, порядок сборки и сценарий презентации
├─ .env.example                               # Образец переменных окружения (ключи LLM/TTS/STT/Redis)
├─ docker-compose.yml                         # (опц.) Redis, Nginx reverse-proxy для локального стенда
├─ scripts/
│  ├─ load_static_audio.py                    # Оффлайн-прогрев кэша: режет WAV на чанки и кладёт в Redis
│  ├─ prepare_embeddings.py                   # Оффлайн-векторизация фраз для IntentClassifier
│  └─ validate_dialogue_map.py                # Валидация dialogue_map.json (ссылки состояний, поля)
├─ configs/
│  ├─ config.yml                              # llm.api_key, models.*, dual_context.*, timeouts
│  ├─ prompts.yml                             # system_prompt, response_format_instruction, summarization_prompt
│  ├─ dialogue_map.json                       # Карта сценария для FlowEngine (playlist, transitions, code)
│  └─ intents_backup.pkl                      # Бэкап эмбеддингов интентов (создаёт prepare_embeddings.py)
├─ test_data/
│  └─ example.wav                             # Общий WAV для ручных тестов (переносится в модульные test_data)
├─ infra/
│  ├─ __init__.py
│  ├─ redis_config.py                         # RedisConfig: host/port/db/password, фабрика async-пула
│  ├─ logging.py                              # Единые форматтеры/структурные JSON-логи с trace_id
│  └─ metrics.py                              # MetricsLogger: t1..t6, атрибуты операций, экспорт в лог
├─ domain/
│  ├─ __init__.py
│  ├─ models.py                               # Role, ConversationMessage, LLMStreamChunk, LLMStructuredResponse,
│  │                                           # IntentResult, FlowResult, SessionState, MetricsLog и т.п.
│  └─ interfaces/
│     ├─ __init__.py
│     ├─ cache.py                             # AbstractCache: connect/close, set/get audio/text
│     └─ llm.py                               # Контракты: LLMConnectionManager, AbstractLLMClient,
│                                             # AbstractLLMContext, AbstractConversationManager
├─ cache/
│  ├─ __init__.py
│  ├─ cache.py                                # RedisCacheManager (Singleton), атомарные pipeline-операции
│  └─ test/
│     ├─ manual_test_cache.py                 # Ручной e2e: WAV→чанки→Redis→стрим бенч + проверка всех методов
│     └─ test_data/
│        └─ example.wav                       # Локальная копия для модульного теста cache
├─ llm/
│  ├─ __init__.py
│  ├─ client.py                               # OpenAILLMClient: stream_structured_generate(...)
│  ├─ context.py                              # LLMContext: история, build_prompt/summary, estimate_usage_ratio
│  ├─ dual_context.py                         # DualContextController: пороги, warmup, handover, атомарность
│  ├─ manager.py                              # ConversationManager: фасад, фоновые задачи, кэш суммаризаций
│  └─ test/
│     ├─ manual_test_llm_agent.py             # Ручной тест: искусств. контекст, хендовер, кэш суммаризаций
│     └─ test_data/
│        └─ dialogue_snippet.json             # Мини-диалог для проверки build_prompt/summary
├─ flow_engine/
│  ├─ __init__.py
│  ├─ engine.py                               # FlowEngine v2: process_event(), rollback, exec-валидатор
│  ├─ utils.py                                # check_exec_valid(), безопасная обёртка для exec с валидацией
│  └─ test/
│     ├─ manual_test_flow_engine.py           # Ручной тест: валидация карты, переходы, rollback, ошибки exec
│     └─ test_data/
│        └─ dialogue_map_min.json             # Урезанная карта для сценарных кейсов
├─ intent_classifier/
│  ├─ __init__.py
│  ├─ model_wrapper.py                        # OnnxModelWrapper: загрузка локальной модели, async embed()
│  ├─ repository.py                           # IntentRepository: загрузка бэкапа, выдача векторов/метаданных
│  ├─ entity_extractors.py                    # Простые парсеры: числа/boolean и т.д.
│  ├─ classifier.py                           # IntentClassifier v3 (stateless): classify_intent/find_faq_answer
│  └─ test/
│     ├─ manual_test_intents.py               # Ручной тест: latency<100мс, previous_leader ворота, entity parse
│     └─ test_data/
│        ├─ intents_sample.json               # Подмножество интентов для проверки
│        └─ phrases.txt                       # Тестовые фразы для сравнения/скоринга
├─ stt_yandex/
│  ├─ __init__.py
│  ├─ stt_yandex.py                           # YandexSTTStreamer: очереди audio→response, backpressure, cancel
│  └─ test/
│     ├─ manual_test_stt.py                   # Ручной тест: подача WAV чанками, приём partial/final, остановка
│     └─ test_data/
│        └─ example.wav                       # Локальная копия WAV для STT ручного теста
├─ tts_manager/
│  ├─ __init__.py
│  ├─ connection_manager.py                   # WebSocketConnectionManager: connect/keep-alive/close
│  ├─ manager.py                              # TTSManager: stream_static_text(), start_llm_stream()
│  └─ test/
│     ├─ manual_test_tts.py                   # Ручной тест: HTTP streaming + WS-поток, обрыв/повтор подключения
│     └─ test_data/
│        └─ small_texts.txt                   # Набор коротких текстов для latency-замеров
├─ orchestrator/
│  ├─ __init__.py
│  ├─ orchestrator.py                         # Главный цикл диалога: BOT/USER turns, barge-in, филлеры, метрики
│  └─ test/
│     ├─ manual_test_orchestrator.py          # Ручной e2e: mock STT/LLM/TTS/Cache, сценарии + «не по теме»
│     └─ test_data/
│        ├─ playlist_samples.json             # Плейлисты с cache/filler/tts для проверок
│        └─ barge_in_scenarios.json           # Сценарии перебивания (stop playback → USER_TURN)
└─ webapi/
   ├─ __init__.py
   ├─ main.py                                 # ASGI (Uvicorn): /api/v1/call WebSocket endpoint, фабрика Orchestrator
   └─ test/
      ├─ manual_test_websocket_call.py        # Ручной тест сокета: call_start → run → play_audio → hangup
      └─ test_data/
         └─ init_payload.json                 # Первичное сообщение клиента c call_id/trace_id/initial_vars
```